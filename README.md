ä½¿ç”¨Gradient Descentæ±‚ç·šæ€§æ–¹ç¨‹20180609(åŸå‰µ)
===============================================
é¡Œç›®ä¾†æº:Hung-yi Leeè€å¸«æ©Ÿå™¨å­¸ç¿’ç·šä¸Šèª²ç¨‹
----------------------------------------------
ç¨‹å¼åŸ·è¡Œå‰å…ˆåœ¨ç›¸åŒç›®éŒ„ä¸‹å»ºç«‹è¨˜äº‹æœ¬ *test20180609-1.txt*<br />
å…ˆå‡è¨­ç†æƒ³æ–¹ç¨‹å¼ç‚º *y=3x+10*<br />
è‡ªå·±æ¨¡æ“¬å¯¦éš›å¯¦é©—çµæœå¦‚ä¸‹:<br />
>1   12.5<br />
>2   17<br />
>3   18.2<br />
>4   21.4<br />
>5   27<br />
>6   28.2<br />
>7   32<br />
>8   33.5<br />
>9   39<br />
>10  40<br />

å…¶ä¸­å·¦é‚Šæ•¸å­—ç‚ºè¼¸å…¥å€¼ *x* å³é‚Šæ•¸å­—ç‚ºè¼¸å‡ºå€¼ *y*<br />
åˆ©ç”¨å¦‚ä¸‹ç¨‹å¼èˆ‡æª”æ¡ˆé€²è¡Œé€£çµï¼Œä¸¦ç¢ºèªæ˜¯å¦è®€å–æˆåŠŸ

>ifstream infile;     <br />          
>infile.open("test20180609-1.txt");<br />
>if(!infile.is_open()){      <br />            
>      cout<<"Can't open it";<br />
>      exit(EXIT_FAILURE);<br />
>}<br />

æ¥è‘—å°‡æ–‡å­—æª”å…§è³‡æ–™ä¾åºå­˜å…¥ é™£åˆ— training_data_x[]å’Œ é™£åˆ— training_data_y[]<br />

>for(int i=0;i<=data_members;i++){<br />
>       infile>>training_data_x[i];<br />
> 	infile.get();<br />
> 	infile>>training_data_y[i];<br />
> 	infile.get();<br />
	} <br />




Regression: Output a scalar
======================================
å»ºç«‹æ¨¡å‹  Y =b+w*X<br />
---------------------------------------

å°‡training_data_xçš„è³‡æ–™è¼¸å…¥A set of function å¾—åˆ°çš„è§£ï¼Œèˆ‡training_data_yåšæ¯”è¼ƒ<br />
æ¯”è¼ƒæ–¹æ³•ä¾¿æ˜¯ç®—å‡º*loss function*<br />

>>L(f)=sigma(n=0->9) (Yn-f(Xn))^2<br />

ä»£å…¥ Y = b + w âˆ™ X<br />

>>L(f)=sigma(n=0->9) (Yn-(b+w*Xn))^2<br />

è€ŒGoodness of function å‰‡æ˜¯æ‰¾åˆ°wèˆ‡bå§‹å¾—Loss functionæœ€å°<br />

>> ğœ•ğ¿/ğœ•ğ‘¤ =sigma(n=0->9) 2(Yn-f(Xn))(-Xn) <br />
>> ğœ•ğ¿/ğœ•ğ‘ =sigma(n=0->9) 2(Yn-f(Xn))(-1)

æ¥è‘—ä¿®æ­£<br />

>>ğ‘¤1 â† ğ‘¤0 âˆ’ğœ‚ğœ•ğ¿/ğœ•ğ‘¤|ğ‘¤=ğ‘¤0,ğ‘=ğ‘0<br />
>>ğ‘1 â† ğ‘0 âˆ’ğœ‚ğœ•ğ¿/ğœ•ğ‘|ğ‘¤=ğ‘¤0,ğ‘=ğ‘0<br />

å…¶ä¸­ğœ‚ç‚ºlearning_rate<br />
ä»¥ä¸‹ç‚ºç¨‹å¼çš„å¯¦ç¾:<br />

>for(int j=0;j<=800;j++){            <br /> 
	lossfunction_partial_weight=0;<br />
	lossfunction_partial_bias=0;<br />
	for(int i=0;i<=data_members;i++){<br />
		lossfunction_partial_weight=lossfunction_partial_weight\
		-2*training_data_x[i]*training_data_y[i]\
		+2*current_bias*training_data_x[i]\
		+2*current_weight*training_data_x[i]*training_data_x[i];
	}	          
	for(int i=0;i<=data_members;i++){
		lossfunction_partial_bias=lossfunction_partial_bias\
		-2*training_data_y[i]\
		+2*current_bias\
		+2*current_weight*training_data_x[i];
	}		                
	current_weight=current_weight-learning_rate*lossfunction_partial_weight;
	current_bias=current_bias-learning_rate*lossfunction_partial_bias;
}

å¾åŸæœ¬å…ˆé è¨­<br />

>double current_weight=2;     
>double current_bias=5;  

ç¶“é800æ¬¡çš„Gradient Descentä¿®æ­£å¾Œ<br /> 
å¾—åˆ°çµæœå¦‚ä¸‹:<br /> 

>lossfunction_partial_weight=0.0596218 <br />
>lossfunction_partial_bias=-0.665702 <br />
>current_weight=3.11964 <br />
>current_bias=9.68881 <br />

å¯ä»¥ç™¼ç¾current_weightèˆ‡current_biaså·²ç¶“éå¸¸æ¥è¿‘é è¨­çš„ 3 å’Œ 10 äº†~!<br />


å¾Œè¨˜
================
å¾Œä¾†å¥—å…¥åˆ¥çš„Train data ä½¿ç”¨æ–œç‡æ›´å¤§çš„å‡½æ•¸<br />
å»ç™¼ç¾åˆ©ç”¨Gradientç„¡æ³•æ”¶æ–‚åˆ°è©²å‡½æ•¸<br />
æª¢æŸ¥lossfunction_partial_weightèˆ‡lossfunction_partial_bias <br />
åˆ°å¾Œé¢å®Œå…¨å‘ˆç¾ç™¼æ•£ç‹€æ…‹<br />
å¯èƒ½æ˜¯èµ·å§‹ä½ç½®w0èˆ‡b0è·Ÿå‡½æ•¸å·®è·éå¤§<br />
å…ˆç¨å¾®ä¿®æ”¹w0èˆ‡b0å¾Œï¼Œlossfunction_partial_weightèˆ‡lossfunction_partial_biaså¯ä»¥é€æ¼¸æ”¶æ–‚<br />
ä½†çµæœä»æ˜¯ä¸ç†æƒ³<br />
åœ¨æª¢æŸ¥ç™¼ç¾lossfunction_partial_weightèƒ½å¤ æ”¶æ–‚åˆ°1ä»¥ä¸‹<br />
ä½†lossfunction_partial_biasé›–ç„¶æœ‰æ”¶æ–‚è¶¨å‹¢ï¼Œä½†éå¸¸ç·©æ…¢ï¼Œéœ€è¦å¥½å¹¾å€çš„ä¿®æ­£æ¬¡æ•¸<br />
æ–¼æ˜¯æˆ‘æ”¹æˆè¨­ç«‹2å€‹learning_rate <br />
å…¶ä¸­wçš„learning_rateä¸è®Šï¼Œè€Œbçš„learning_rateèª¿å¤§ <br />
æ¸¬è©¦å¾Œç™¼ç¾å¯ä»¥å®Œå…¨æ”¶æ–‚äº†~ è¡¨ç¤ºè¦æ‡‰ä»˜å¤§æ–œç‡çš„å‡½æ•¸ï¼Œå¯èƒ½è¦è€ƒæ…®ä½¿ç”¨2å€‹learning_rate <br />
ç•¶ç„¶é€™ç¨®è¿‘ä¼¼æ–¹æ³•é‚„æœ‰è¨±å¤šå•é¡Œï¼Œç­‰ä¹‹å¾Œå­¸ç¿’æ›´æ·±å…¥å¾Œå†ä¾†ä¿®æ­£~~



  
  
<br />
