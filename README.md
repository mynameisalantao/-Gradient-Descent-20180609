ä½¿ç”¨Gradient Descentæ±‚ç·šæ€§æ–¹ç¨‹20180609(åŸå‰µ)
===============================================
é¡Œç›®ä¾†æº:Hung-yi Leeè€å¸«æ©Ÿå™¨å­¸ç¿’ç·šä¸Šèª²ç¨‹
----------------------------------------------
ç¨‹å¼åŸ·è¡Œå‰å…ˆåœ¨ç›¸åŒç›®éŒ„ä¸‹å»ºç«‹è¨˜äº‹æœ¬ *test20180609-1.txt*<br />
å…ˆå‡è¨­ç†æƒ³æ–¹ç¨‹å¼ç‚º *y=3x+10*<br />
è‡ªå·±æ¨¡æ“¬å¯¦éš›å¯¦é©—çµæœå¦‚ä¸‹:<br />
>1   12.5<br />
>2   17<br />
>3   18.2<br />
>4   21.4<br />
>5   27<br />
>6   28.2<br />
>7   32<br />
>8   33.5<br />
>9   39<br />
>10  40<br />
å…¶ä¸­å·¦é‚Šæ•¸å­—ç‚ºè¼¸å…¥å€¼ *x* å³é‚Šæ•¸å­—ç‚ºè¼¸å…¥å€¼ *y*<br />
åˆ©ç”¨å¦‚ä¸‹ç¨‹å¼èˆ‡æª”æ¡ˆé€²è¡Œé€£çµï¼Œä¸¦ç¢ºèªæ˜¯å¦è®€å–æˆåŠŸ
-----------------------------------------------
>ifstream infile;     <br />          
>infile.open("test20180609-1.txt");<br />
>if(!infile.is_open()){      <br />            
>      cout<<"Can't open it";<br />
>      exit(EXIT_FAILURE);<br />
>}<br />
æ¥è‘—å°‡æ–‡å­—æª”å…§è³‡æ–™ä¾åºå­˜å…¥ é™£åˆ— training_data_x[]å’Œ é™£åˆ— training_data_y[]<br />
-------------------------------------------------------------------------------
>for(int i=0;i<=data_members;i++){<br />
>       infile>>training_data_x[i];<br />
> 	infile.get();<br />
> 	infile>>training_data_y[i];<br />
> 	infile.get();<br />
	} <br />




Regression: Output a scalar
======================================
å»ºç«‹æ¨¡å‹  Y =b+w*X<br />
å°‡training_data_xçš„è³‡æ–™è¼¸å…¥A set of function å¾—åˆ°çš„è§£ï¼Œèˆ‡training_data_yåšæ¯”è¼ƒ<br />
æ¯”è¼ƒæ–¹æ³•ä¾¿æ˜¯ç®—å‡ºloss function<br />
>>L(f)=sigma(n=0~9) (Yn-f(Xn))^2<br />
ä»£å…¥ Y = b + w âˆ™ X<br />
>>L(f)=sigma(n=0~9) (Yn-(b+w*Xn))^2<br />
è€ŒGoodness of function å‰‡æ˜¯æ‰¾åˆ°wèˆ‡bå§‹å¾—Loss functionæœ€å°<br />
>>ğœ•ğ¿/ğœ•ğ‘¤=sigma(n=0~9) 2(Yn-f(Xn))(-Xn)<br />
>>ğœ•ğ¿/ğœ•ğ‘=sigma(n=0~9) 2(Yn-f(Xn))(-1)<br />  
æ¥è‘—ä¿®æ­£<br />
>>ğ‘¤1 â† ğ‘¤0 âˆ’ğœ‚ğœ•ğ¿/ğœ•ğ‘¤|ğ‘¤=ğ‘¤0,ğ‘=ğ‘0<br />
>>ğ‘1 â† ğ‘0 âˆ’ğœ‚ğœ•ğ¿/ğœ•ğ‘|ğ‘¤=ğ‘¤0,ğ‘=ğ‘0<br />
å…¶ä¸­ğœ‚ç‚ºlearning_rate<br />
ä»¥ä¸‹ç‚ºç¨‹å¼çš„å¯¦ç¾:<br />
>for(int j=0;j<=800;j++){            <br /> 
	lossfunction_partial_weight=0;<br />
	lossfunction_partial_bias=0;<br />
	for(int i=0;i<=data_members;i++){<br />
		lossfunction_partial_weight=lossfunction_partial_weight\<br />
		-2*training_data_x[i]*training_data_y[i]\<br />
		+2*current_bias*training_data_x[i]\<br />
		+2*current_weight*training_data_x[i]*training_data_x[i];<br />
	}	            <br />
	for(int i=0;i<=data_members;i++){<br />
		lossfunction_partial_bias=lossfunction_partial_bias\<br />
		-2*training_data_y[i]\<br />
		+2*current_bias\<br />
		+2*current_weight*training_data_x[i];<br />
	}		                <br />
	current_weight=current_weight-learning_rate*lossfunction_partial_weight;<br />
	current_bias=current_bias-learning_rate*lossfunction_partial_bias;<br />
}<br />
å¾åŸæœ¬å…ˆé è¨­<br />
>double current_weight=2;       <br />   
>double current_bias=5;  <br />
ç¶“é800æ¬¡çš„Gradient Descentä¿®æ­£å¾Œ<br /> 
å¾—åˆ°çµæœå¦‚ä¸‹:<br /> 
>lossfunction_partial_weight=0.0596218<br />
>lossfunction_partial_bias=-0.665702<br />
>current_weight=3.11964<br />
>current_bias=9.68881<br />
å¯ä»¥ç™¼ç¾current_weightèˆ‡current_biaså·²ç¶“éå¸¸æ¥è¿‘é è¨­çš„ 3 å’Œ 10 äº†~!<br />






  
  
<br />
