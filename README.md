ä½¿ç”¨Gradient Descentæ±‚ç·šæ€§æ–¹ç¨‹20180609(åŸå‰µ)
===============================================
æƒ³æ³•ä¾†æº:Hung-yi Leeè€å¸«æ©Ÿå™¨å­¸ç¿’ç·šä¸Šèª²ç¨‹
----------------------------------------------
ç¨‹å¼èªè¨€:ä½¿ç”¨C++   æ’°å¯«:Alan Tao  åƒè€ƒ:ç„¡
----------------------------------------------
ç¨‹å¼åŸ·è¡Œå‰å…ˆåœ¨ç›¸åŒç›®éŒ„ä¸‹å»ºç«‹è¨˜äº‹æœ¬ *test20180609-1.txt*<br />
å…ˆå‡è¨­ç†æƒ³æ–¹ç¨‹å¼ç‚º *y=3x+10*<br />
è‡ªå·±æ¨¡æ“¬å¯¦éš›å¯¦é©—çµæœå¦‚ä¸‹:<br />
<pre><code>1   12.5
2   17
3   18.2
4   21.4
5   27
6   28.2
7   32
8   33.5
9   39
10  40</pre></code>

å…¶ä¸­å·¦é‚Šæ•¸å­—ç‚ºè¼¸å…¥å€¼ *x* å³é‚Šæ•¸å­—ç‚ºè¼¸å‡ºå€¼ *y*<br />
åˆ©ç”¨å¦‚ä¸‹ç¨‹å¼èˆ‡æª”æ¡ˆé€²è¡Œé€£çµï¼Œä¸¦ç¢ºèªæ˜¯å¦è®€å–æˆåŠŸ

<pre><code>ifstream infile;           
infile.open("test20180609-1.txt");
if(!infile.is_open()){                
      cout<<"Can't open it";
      exit(EXIT_FAILURE);
}</pre></code>

æ¥è‘—å°‡æ–‡å­—æª”å…§è³‡æ–™ä¾åºå­˜å…¥ é™£åˆ— training_data_x[]å’Œ é™£åˆ— training_data_y[]

<pre><code>for(int i=0;i<=data_members;i++){
       infile>>training_data_x[i];
 	infile.get();
 	infile>>training_data_y[i];
 	infile.get();
	} </pre></code>




Regression: Output a scalar
======================================
å»ºç«‹æ¨¡å‹  Y =b+w*X
---------------------------------------

å°‡training_data_xçš„è³‡æ–™è¼¸å…¥A set of function å¾—åˆ°çš„è§£ï¼Œèˆ‡training_data_yåšæ¯”è¼ƒ<br />
æ¯”è¼ƒæ–¹æ³•ä¾¿æ˜¯ç®—å‡º*loss function*

<pre><code>L(f)=sigma(n=0->9) (Yn-f(Xn))^2</pre></code>

ä»£å…¥ Y = b + w âˆ™ X<br />

<pre><code>L(f)=sigma(n=0->9) (Yn-(b+w*Xn))^2</pre></code>

è€ŒGoodness of function å‰‡æ˜¯æ‰¾åˆ°wèˆ‡bå§‹å¾—Loss functionæœ€å°

<pre><code>ğœ•ğ¿/ğœ•ğ‘¤ =sigma(n=0->9) 2(Yn-f(Xn))(-Xn) 
ğœ•ğ¿/ğœ•ğ‘ =sigma(n=0->9) 2(Yn-f(Xn))(-1)</pre></code>

æ¥è‘—ä¿®æ­£

<pre><code>ğ‘¤1 â† ğ‘¤0 âˆ’ğœ‚ğœ•ğ¿/ğœ•ğ‘¤|ğ‘¤=ğ‘¤0,ğ‘=ğ‘0
ğ‘1 â† ğ‘0 âˆ’ğœ‚ğœ•ğ¿/ğœ•ğ‘|ğ‘¤=ğ‘¤0,ğ‘=ğ‘0</pre></code>
å…¶ä¸­ğœ‚ç‚ºlearning_rate<br />
ä»¥ä¸‹ç‚ºç¨‹å¼çš„å¯¦ç¾:

<pre><code>for(int j=0;j<=800;j++){            
	lossfunction_partial_weight=0;
	lossfunction_partial_bias=0;
	for(int i=0;i<=data_members;i++){
		lossfunction_partial_weight=lossfunction_partial_weight\
		-2*training_data_x[i]*training_data_y[i]\
		+2*current_bias*training_data_x[i]\
		+2*current_weight*training_data_x[i]*training_data_x[i];
	}	          
	for(int i=0;i<=data_members;i++){
		lossfunction_partial_bias=lossfunction_partial_bias\
		-2*training_data_y[i]\
		+2*current_bias\
		+2*current_weight*training_data_x[i];
	}		                
	current_weight=current_weight-learning_rate*lossfunction_partial_weight;
	current_bias=current_bias-learning_rate*lossfunction_partial_bias;
}</pre></code>

å¾åŸæœ¬å…ˆé è¨­<br />

<pre><code>double current_weight=2;     
double current_bias=5;  </pre></code>

ç¶“é800æ¬¡çš„Gradient Descentä¿®æ­£å¾Œ<br /> 
å¾—åˆ°çµæœå¦‚ä¸‹:<br /> 

<pre><code>lossfunction_partial_weight=0.0596218 
lossfunction_partial_bias=-0.665702 
current_weight=3.11964 
current_bias=9.68881 </pre></code>

å¯ä»¥ç™¼ç¾current_weightèˆ‡current_biaså·²ç¶“éå¸¸æ¥è¿‘é è¨­çš„ 3 å’Œ 10 äº†~!<br />


å¾Œè¨˜
================
å¾Œä¾†å¥—å…¥åˆ¥çš„Train data ä½¿ç”¨æ–œç‡æ›´å¤§çš„å‡½æ•¸<br />
å»ç™¼ç¾åˆ©ç”¨Gradientç„¡æ³•æ”¶æ–‚åˆ°è©²å‡½æ•¸<br />
æª¢æŸ¥lossfunction_partial_weightèˆ‡lossfunction_partial_bias <br />
åˆ°å¾Œé¢å®Œå…¨å‘ˆç¾ç™¼æ•£ç‹€æ…‹<br />
å¯èƒ½æ˜¯èµ·å§‹ä½ç½®w0èˆ‡b0è·Ÿå‡½æ•¸å·®è·éå¤§<br />
å…ˆç¨å¾®ä¿®æ”¹w0èˆ‡b0å¾Œï¼Œlossfunction_partial_weightèˆ‡lossfunction_partial_biaså¯ä»¥é€æ¼¸æ”¶æ–‚<br />
ä½†çµæœä»æ˜¯ä¸ç†æƒ³<br />
åœ¨æª¢æŸ¥ç™¼ç¾lossfunction_partial_weightèƒ½å¤ æ”¶æ–‚åˆ°1ä»¥ä¸‹<br />
ä½†lossfunction_partial_biasé›–ç„¶æœ‰æ”¶æ–‚è¶¨å‹¢ï¼Œä½†éå¸¸ç·©æ…¢ï¼Œéœ€è¦å¥½å¹¾å€çš„ä¿®æ­£æ¬¡æ•¸<br />
æ–¼æ˜¯æˆ‘æ”¹æˆè¨­ç«‹2å€‹learning_rate <br />
å…¶ä¸­wçš„learning_rateä¸è®Šï¼Œè€Œbçš„learning_rateèª¿å¤§ <br />
æ¸¬è©¦å¾Œç™¼ç¾å¯ä»¥å®Œå…¨æ”¶æ–‚äº†~ è¡¨ç¤ºè¦æ‡‰ä»˜å¤§æ–œç‡çš„å‡½æ•¸ï¼Œå¯èƒ½è¦è€ƒæ…®ä½¿ç”¨2å€‹learning_rate <br />
ç•¶ç„¶é€™ç¨®è¿‘ä¼¼æ–¹æ³•é‚„æœ‰è¨±å¤šå•é¡Œï¼Œç­‰ä¹‹å¾Œå­¸ç¿’æ›´æ·±å…¥å¾Œå†ä¾†ä¿®æ­£~~ <br />
PS.ä¿®æ­£å¾Œç¨‹å¼ç¢¼å¯åƒè€ƒVer2.0




  
  
<br />
